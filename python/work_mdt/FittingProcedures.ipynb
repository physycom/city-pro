{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration Usage Fitting Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FittingProcedures\n",
    "from FittingProcedures import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import os\n",
    "from fit_dictionaries import *\n",
    "from fit_tricks_to_compare_different_conditions import *\n",
    "from Fit_spcific_cases import *\n",
    "from FittingProcedures import *\n",
    "from fit_plots import *\n",
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.ticker as ticker\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "# Import functions from existing modules\n",
    "from fit_tricks_to_compare_different_conditions import enrich_vector_to_length, from_feature_to_nin_bin_range, from_data_to_cut_distribution\n",
    "from FittingProcedures import compare_exponential_power_law_from_xy, fit_expo\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions average filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users_if_one_point_out_of_polygon(Fcm,df_cut_traj,col_user_fcm,col_user_cut):\n",
    "    \"\"\"\n",
    "        Filter the trajectories that are not completely inside the polygon.\n",
    "\n",
    "    \"\"\"\n",
    "    users_2_filter = df_cut_traj.select(pl.col(col_user_cut)).unique().to_series().to_list()\n",
    "    return Fcm.filter(pl.col(col_user_fcm).is_in(users_2_filter))   \n",
    "def load_data(fcm_dir: str, date_str: str, filter_space: pl.Expr, filter_time: pl.Expr) -> Optional[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load FCM data for a specific date.\n",
    "    \n",
    "    Args:\n",
    "        fcm_dir: Directory containing FCM data files\n",
    "        date_str: Date string in format YYYY-MM-DD\n",
    "        filter_space: Polars expression for filtering spatial data\n",
    "        filter_time: Polars expression for filtering temporal data\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame or None if loading fails\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(fcm_dir, f\"bologna_mdt_{date_str}_{date_str}_fcm.csv\")\n",
    "    try:\n",
    "        fcm = pl.read_csv(file_path)\n",
    "        filtered_fcm = fcm.filter(filter_space, filter_time)\n",
    "        \n",
    "        if filtered_fcm.height == 0:\n",
    "            print(f\"No data for {date_str} after applying filters\")\n",
    "            return None\n",
    "        \n",
    "        return filtered_fcm\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or filtering data for {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_inset_means(day_2_mean: Dict[str, float], \n",
    "                    dates: List[str],\n",
    "                    feature: str,\n",
    "                    ax_inset: plt.Axes) -> plt.Axes:\n",
    "    \"\"\"\n",
    "    Plot an inset showing the mean values for each day and the average across days.\n",
    "    \n",
    "    Args:\n",
    "        day_2_mean: Dictionary mapping days to mean values\n",
    "        dates: List of date strings\n",
    "        feature: Feature name ('time_hours' or 'lenght_km')\n",
    "        ax_inset: Matplotlib axes object for the inset plot\n",
    "        \n",
    "    Returns:\n",
    "        Updated matplotlib axes object\n",
    "    \"\"\"\n",
    "    # Initialize array for average calculation\n",
    "    average_mean_over_day = 0.0\n",
    "    valid_days = 0\n",
    "    \n",
    "    # Plot each day's mean\n",
    "    for date_str in dates:\n",
    "        if date_str in day_2_mean and day_2_mean[date_str] != 0:\n",
    "            x_means_day = day_2_mean[date_str]\n",
    "            average_mean_over_day += x_means_day\n",
    "            valid_days += 1\n",
    "            \n",
    "            # Plot daily value\n",
    "            ax_inset.scatter([0], [x_means_day], s=25)\n",
    "    \n",
    "    # Calculate and plot average\n",
    "    if valid_days > 0:\n",
    "        average_mean_over_day /= valid_days\n",
    "        ax_inset.scatter([0], [average_mean_over_day], marker=\"*\", s=45, color='red')\n",
    "    \n",
    "    # Configure axes\n",
    "    ax_inset.set_xticks([0])\n",
    "    ax_inset.set_xticklabels([\"\"])\n",
    "    \n",
    "    # Set appropriate y-label based on feature\n",
    "    if feature == \"time_hours\":\n",
    "        ax_inset.set_ylabel(r\"$\\langle t \\rangle$ (h)\", fontsize=12)        \n",
    "    else:\n",
    "        ax_inset.set_ylabel(r\"$\\langle l \\rangle$ (km)\", fontsize=12)\n",
    "    \n",
    "    return ax_inset\n",
    "\n",
    "def process_feature_fit(feature: str,\n",
    "                        x: np.ndarray, \n",
    "                        average_y_over_day: np.ndarray,\n",
    "                        average_x_mean_over_day: float,\n",
    "                        cut_length: float = 4) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a feature and determine the best fit (exponential or power law).\n",
    "    \n",
    "    Args:\n",
    "        feature: Feature name ('time_hours' or 'lenght_km')\n",
    "        x: x values\n",
    "        average_y_over_day: y values averaged over all days\n",
    "        average_x_mean_over_day: Mean value averaged over all days\n",
    "        cut_length: Maximum length to consider for length distributions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing fit results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make sure x and y have the same dimensions\n",
    "        if len(x) != len(average_y_over_day):\n",
    "            print(f\"Warning: x and y dimensions don't match. x: {len(x)}, y: {len(average_y_over_day)}\")\n",
    "            # Match the lengths by truncating to the shorter one\n",
    "            max_len = max(len(x), len(average_y_over_day))\n",
    "            x = enrich_vector_to_length(x, max_len)\n",
    "            average_y_over_day = enrich_vector_to_length(average_y_over_day)\n",
    "            print(f\"Adjusted to length {max_len} for both x and y.\")\n",
    "        \n",
    "        if \"leng\" in feature:\n",
    "            # Apply length cutoff for length feature\n",
    "            mask = np.array(x) <= cut_length\n",
    "            x_masked = np.array(x)[mask]\n",
    "            average_y_masked = np.array(average_y_over_day)[mask]\n",
    "            \n",
    "            print(f\"Feature: {feature}, size x: {len(x_masked)}, size average_y_over_day: {len(average_y_masked)}\")\n",
    "            \n",
    "            # Fit exponential only for length\n",
    "            A_exp, beta_exp, exp_, error_exp, R2_exp, bins_plot = fit_expo(x_masked, average_y_masked)\n",
    "            error_pl = 1e10  # Force exponential fit for length\n",
    "            pl_ = None\n",
    "            alpha_pl = None\n",
    "        else:\n",
    "            # Compare exponential and power law fits for time\n",
    "            A_exp, beta_exp, exp_, error_exp, R2_exp, A_pl, alpha_pl, pl_, error_pl, R2_pl, bins_plot = \\\n",
    "                compare_exponential_power_law_from_xy(np.array(x), np.array(average_y_over_day))\n",
    "        \n",
    "        # Determine the best fit\n",
    "        if error_exp < error_pl:\n",
    "            # Exponential is better\n",
    "            fit_result = {\n",
    "                \"fit_type\": \"exponential\",\n",
    "                \"y_fit\": exp_,\n",
    "                \"parameter\": beta_exp,\n",
    "                \"parameter_name\": \"β\",\n",
    "                \"fit_param\": {\"expo\": beta_exp, \"pl\": None}\n",
    "            }\n",
    "        else:\n",
    "            # Power law is better\n",
    "            fit_result = {\n",
    "                \"fit_type\": \"power law\",\n",
    "                \"y_fit\": pl_,\n",
    "                \"parameter\": alpha_pl,\n",
    "                \"parameter_name\": \"α\",\n",
    "                \"fit_param\": {\"expo\": None, \"pl\": alpha_pl}\n",
    "            }\n",
    "        \n",
    "        # Add common attributes\n",
    "        fit_result.update({\n",
    "            \"average_y\": average_y_over_day,\n",
    "            \"x_mean\": average_x_mean_over_day,\n",
    "            \"A_exp\": A_exp,\n",
    "            \"exp_\": exp_,\n",
    "            \"A_pl\": A_pl if \"A_pl\" in locals() else None,\n",
    "            \"pl_\": pl_\n",
    "        })\n",
    "        \n",
    "        return fit_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature fit for {feature}: {e}\")\n",
    "        return {\"fit_type\": None, \"error\": str(e)}\n",
    "\n",
    "def plot_feature_distribution(feature: str, \n",
    "                            fit_result: Dict[str, Any],\n",
    "                            x: np.ndarray,\n",
    "                            dates: List[str],\n",
    "                            feature_2_day_2_x: Dict,\n",
    "                            feature_2_day_2_y: Dict,\n",
    "                            feature_2_day_2_mean: Dict,\n",
    "                            enriched_vector_length: int,\n",
    "                            plot_dir: str,\n",
    "                            range_time_hours: List[float],\n",
    "                            range_length_km: List[float],\n",
    "                            case: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Plot the distribution for a feature with its best fit.\n",
    "    \n",
    "    Args:\n",
    "        feature: Feature name ('time_hours' or 'lenght_km')\n",
    "        fit_result: Dictionary containing fit results\n",
    "        x: x values for plotting\n",
    "        dates: List of date strings\n",
    "        feature_2_day_2_x: Dictionary of x values by day\n",
    "        feature_2_day_2_y: Dictionary of y values by day\n",
    "        feature_2_day_2_mean: Dictionary of mean values by day\n",
    "        enriched_vector_length: Target length for enriched vectors\n",
    "        plot_dir: Directory to save plots\n",
    "        range_time_hours: Range for time hours [min, max]\n",
    "        range_length_km: Range for length km [min, max]\n",
    "        case: Suffix for output filename\n",
    "    \"\"\"\n",
    "    if fit_result.get(\"fit_type\") is None:\n",
    "        print(f\"No valid fit for {feature}, skipping plot.\")\n",
    "        return\n",
    "        \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    \n",
    "    # Plot each day's data points\n",
    "    for day in dates:            \n",
    "        try:\n",
    "            day_x = feature_2_day_2_x[feature][day]\n",
    "            day_y = feature_2_day_2_y[feature][day]\n",
    "            \n",
    "            # Normalize\n",
    "            day_y_normalized = day_y / np.sum(day_y) if np.sum(day_y) > 0 else day_y\n",
    "            ax.scatter(day_x, day_y_normalized, label=day, alpha=0.5, s=20)\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting data for day {day}: {e}\")\n",
    "    \n",
    "    # Plot the fit curve\n",
    "    if fit_result[\"fit_type\"] == \"exponential\" and \"exp_\" in fit_result:\n",
    "        # Normalize for plotting\n",
    "        exp_norm = fit_result[\"exp_\"] / np.sum(fit_result[\"exp_\"]) if np.sum(fit_result[\"exp_\"]) > 0 else fit_result[\"exp_\"]\n",
    "        \n",
    "        # Scale exponential for length feature\n",
    "        if \"len\" in feature:\n",
    "            avg_y = fit_result[\"average_y\"]\n",
    "            if len(avg_y) > 0 and len(exp_norm) > 0 and exp_norm[0] != 0:\n",
    "                mult_factor = avg_y[0] / exp_norm[0]\n",
    "                exp_norm = exp_norm * mult_factor\n",
    "        \n",
    "        # Plot fit\n",
    "        ax.plot(x, enrich_vector_to_length(exp_norm,enriched_vector_length), label=\"Exponential fit\", linewidth=2, color='red')\n",
    "        \n",
    "    elif fit_result[\"fit_type\"] == \"power law\" and \"pl_\" in fit_result:\n",
    "        # Normalize for plotting\n",
    "        pl_norm = fit_result[\"pl_\"] / np.sum(fit_result[\"pl_\"]) if np.sum(fit_result[\"pl_\"]) > 0 else fit_result[\"pl_\"]\n",
    "        \n",
    "        # Plot fit\n",
    "        ax.plot(x, enrich_vector_to_length(pl_norm,enriched_vector_length), label=\"Power law fit\", linewidth=2, color='blue')\n",
    "    \n",
    "    # Set plot properties\n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    if feature == \"time_hours\":\n",
    "        ax.set_xlabel(r\"$t$ (h)\", fontsize=25)\n",
    "        ax.set_ylabel(r\"$P(t)$\", fontsize=25)\n",
    "        ax.set_xlim(range_time_hours[0], range_time_hours[1])\n",
    "    else:\n",
    "        ax.set_xlabel(r\"$l$ (km)\", fontsize=25)\n",
    "        ax.set_ylabel(r\"$P(l)$\", fontsize=25)\n",
    "        ax.set_xlim(range_length_km[0], range_length_km[1])\n",
    "        \n",
    "    # Add inset with mean values\n",
    "    ax_inset = inset_axes(ax, width=\"30%\", height=\"25%\", loc=\"upper right\")\n",
    "    ax_inset = plot_inset_means(\n",
    "        feature_2_day_2_mean[feature], \n",
    "        dates,\n",
    "        feature, \n",
    "        ax_inset\n",
    "    )\n",
    "    \n",
    "    # Add title with fit information\n",
    "    ax.yaxis.set_major_locator(ticker.LogLocator(numticks=4))  # Request 4 ticks (usually gives at least 3)\n",
    "    ax.yaxis.set_minor_locator(ticker.LogLocator(subs='auto', numticks=10))  # Add minor ticks\n",
    "\n",
    "    # Optional: Format the tick labels to be more readable\n",
    "    ax.yaxis.set_major_formatter(ticker.LogFormatterSciNotation(labelOnlyBase=False))\n",
    "    # Adjust legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), \n",
    "             loc='upper right', bbox_to_anchor=(0.98, 0.98),\n",
    "             fontsize=12, framealpha=0.8)\n",
    "    \n",
    "    # Save figure\n",
    "#    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, f\"{feature}_distribution_averaged_over_days{case}.png\"), dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def compute_distributions(features: List[str],\n",
    "                        dates: List[str],\n",
    "                        fcm_dir: str,\n",
    "                        filter_space: pl.Expr,\n",
    "                        filter_time: pl.Expr,\n",
    "                        bin_size_time_hours: float,\n",
    "                        bin_size_length_km: float,\n",
    "                        range_time_hours: List[float] = [0.1, 2],\n",
    "                        range_length_km: List[float] = [0.1, 10],\n",
    "                        enriched_vector_length: int = 50) -> Tuple[Dict, Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Compute distributions of time and length for each feature and each day.\n",
    "    \n",
    "    Args:\n",
    "        features: List of features to analyze ('time_hours', 'lenght_km')\n",
    "        dates: List of date strings to process\n",
    "        fcm_dir: Directory containing FCM data files\n",
    "        filter_space: Polars expression for filtering spatial data\n",
    "        filter_time: Polars expression for filtering temporal data\n",
    "        bin_size_time_hours: Bin size for time histograms\n",
    "        bin_size_length_km: Bin size for length histograms\n",
    "        range_time_hours: Range for time distributions [min, max]\n",
    "        range_length_km: Range for length distributions [min, max]\n",
    "        enriched_vector_length: Target length for interpolated vectors\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of dictionaries: (feature_2_day_2_x, feature_2_day_2_y, \n",
    "                               feature_2_day_2_y_fit, feature_2_day_2_mean)\n",
    "    \"\"\"\n",
    "    # Initialize result dictionaries\n",
    "    feature_2_day_2_x = {feature: {day: [] for day in dates} for feature in features}\n",
    "    feature_2_day_2_y = {feature: {day: [] for day in dates} for feature in features}\n",
    "    feature_2_day_2_y_fit = {feature: {day: [] for day in dates} for feature in features}\n",
    "    feature_2_day_2_mean = {feature: {day: 0 for day in dates} for feature in features}\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature in features:\n",
    "        print(f\"Feature: {feature}\")\n",
    "        \n",
    "        # Process each date\n",
    "        for date_str in dates:\n",
    "            print(f\"Date: {date_str}\")\n",
    "            \n",
    "            # Read FCM data and apply filters\n",
    "            fcm_data = load_data(fcm_dir, date_str, filter_space, filter_time)\n",
    "            if fcm_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract feature data\n",
    "            feature_data = fcm_data[feature].to_list()\n",
    "            \n",
    "            # Compute fit and distribution\n",
    "            try:\n",
    "                # Choose the range for the bins\n",
    "                bins, bin_range = from_feature_to_nin_bin_range(\n",
    "                    feature, \n",
    "                    range_time_hours, \n",
    "                    bin_size_time_hours, \n",
    "                    range_length_km, \n",
    "                    bin_size_length_km\n",
    "                )\n",
    "                \n",
    "                # Get histogram data\n",
    "                x, n = from_data_to_cut_distribution(feature_data, bins, bin_range)\n",
    "                \n",
    "                # Calculate mean\n",
    "                x_mean = np.nanmean(feature_data)\n",
    "                \n",
    "                # Generate enriched vectors for better visualization\n",
    "                x_enriched = enrich_vector_to_length(x, enriched_vector_length)\n",
    "                n_enriched = enrich_vector_to_length(n, enriched_vector_length)\n",
    "                \n",
    "                # Try fitting\n",
    "                try:\n",
    "                    A_exp, beta_exp, exp_, error_exp, R2_exp, A_pl, alpha_pl, pl_, error_pl, R2_pl, bins_plot = \\\n",
    "                        compare_exponential_power_law_from_xy(x, n)\n",
    "                        \n",
    "                    # Choose best fit\n",
    "                    if error_exp < error_pl:\n",
    "                        y_fit = enrich_vector_to_length(exp_, enriched_vector_length)\n",
    "                        is_exp = True\n",
    "                    else:\n",
    "                        y_fit = enrich_vector_to_length(pl_, enriched_vector_length)\n",
    "                        is_exp = False\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting distributions for {date_str}, feature {feature}: {e}\")\n",
    "                    y_fit = np.zeros_like(n_enriched)\n",
    "                \n",
    "                print(f\"size x: {len(x_enriched)}, size n: {len(n_enriched)}, size y_fit: {len(y_fit)}, <x>: {x_mean:.4f}\")\n",
    "                \n",
    "                # Store results\n",
    "                feature_2_day_2_y[feature][date_str] = n_enriched\n",
    "                feature_2_day_2_x[feature][date_str] = x_enriched\n",
    "                feature_2_day_2_mean[feature][date_str] = x_mean\n",
    "                feature_2_day_2_y_fit[feature][date_str] = y_fit\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error computing distributions for {date_str}, feature {feature}: {e}\")\n",
    "                \n",
    "    return feature_2_day_2_x, feature_2_day_2_y, feature_2_day_2_y_fit, feature_2_day_2_mean\n",
    "\n",
    "def plot_distributions(features: List[str],\n",
    "                      dates: List[str],\n",
    "                      feature_2_day_2_x: Dict,\n",
    "                      feature_2_day_2_y: Dict,\n",
    "                      feature_2_day_2_mean: Dict,\n",
    "                      enriched_vector_length: int,\n",
    "                      plot_dir: str,\n",
    "                      range_time_hours: List[float],\n",
    "                      range_length_km: List[float],\n",
    "                      cut_length: float = 4,\n",
    "                      case: str = \"\") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Plot time and length distributions averaged over all days.\n",
    "    \n",
    "    Args:\n",
    "        features: List of features to analyze\n",
    "        dates: List of date strings\n",
    "        feature_2_day_2_x: Dictionary of x values by feature and day\n",
    "        feature_2_day_2_y: Dictionary of y values by feature and day\n",
    "        feature_2_day_2_mean: Dictionary of mean values by feature and day\n",
    "        enriched_vector_length: Target length for enriched vectors\n",
    "        plot_dir: Directory to save plots\n",
    "        range_time_hours: Range for time hours [min, max]\n",
    "        range_length_km: Range for length km [min, max]\n",
    "        cut_length: Maximum length to consider for length distributions\n",
    "        case: Suffix for output filenames\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of fit parameters for each feature\n",
    "    \"\"\"\n",
    "    # Initialize result dictionaries\n",
    "    feature_2_fit_param = {feature: {\"expo\": None, \"pl\": None} for feature in features}\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature in features:\n",
    "        # Initialize average arrays\n",
    "        average_y_over_day = np.zeros(enriched_vector_length)\n",
    "        average_x_mean_over_day = 0\n",
    "        valid_days = 0\n",
    "        x = None\n",
    "        \n",
    "        # Calculate average across days\n",
    "        for day in dates:\n",
    "            if not feature_2_day_2_x[feature].get(day) or not feature_2_day_2_y[feature].get(day):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                day_x = feature_2_day_2_x[feature][day] \n",
    "                day_y = feature_2_day_2_y[feature][day]\n",
    "                \n",
    "                if x is None:\n",
    "                    x = day_x\n",
    "                    \n",
    "                # Make sure data is properly aligned\n",
    "                if len(day_x) == len(day_y) == enriched_vector_length:\n",
    "                    average_y_over_day += np.array(day_y)\n",
    "                    average_x_mean_over_day += feature_2_day_2_mean[feature][day]\n",
    "                    valid_days += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping day {day} due to mismatched dimensions. x: {len(day_x)}, y: {len(day_y)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing day {day}: {e}\")\n",
    "        \n",
    "        # Skip if no valid days found\n",
    "        if valid_days == 0:\n",
    "            print(f\"No valid data found for {feature}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate averages\n",
    "        average_y_over_day /= valid_days\n",
    "        average_x_mean_over_day /= valid_days\n",
    "        \n",
    "        # Process fit for this feature\n",
    "        fit_result = process_feature_fit(\n",
    "            feature,\n",
    "            x,\n",
    "            average_y_over_day,\n",
    "            average_x_mean_over_day,\n",
    "            cut_length\n",
    "        )\n",
    "        \n",
    "        # Store fit parameters\n",
    "        if fit_result.get(\"fit_param\"):\n",
    "            feature_2_fit_param[feature] = fit_result[\"fit_param\"]\n",
    "        \n",
    "        # Plot the distribution\n",
    "        plot_feature_distribution(\n",
    "            feature, \n",
    "            fit_result, \n",
    "            x,\n",
    "            dates,\n",
    "            feature_2_day_2_x,\n",
    "            feature_2_day_2_y,\n",
    "            feature_2_day_2_mean,\n",
    "            enriched_vector_length,\n",
    "            plot_dir,\n",
    "            range_time_hours,\n",
    "            range_length_km,\n",
    "            case\n",
    "        )\n",
    "    \n",
    "    # Save fit parameters\n",
    "    try:\n",
    "        with open(os.path.join(plot_dir, f\"fit_param_aggregated_length_and_time{case}.json\"), \"w\") as f:\n",
    "            json.dump(feature_2_fit_param, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving fit parameters: {e}\")\n",
    "        \n",
    "    return feature_2_fit_param\n",
    "\n",
    "def run_analysis(config: Dict[str, Any]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Run the full analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing configuration parameters\n",
    "            - features: List of features to analyze\n",
    "            - dates: List of date strings\n",
    "            - fcm_dir: Directory containing FCM data files\n",
    "            - plot_dir: Directory to save plots\n",
    "            - filter_params: Dictionary of filter parameters\n",
    "            - distribution_params: Dictionary of distribution parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of fit parameters for each feature\n",
    "    \"\"\"\n",
    "    # Extract configuration parameters\n",
    "    features = config.get('features', [\"time_hours\", \"lenght_km\"])\n",
    "    dates = config.get('dates', [])\n",
    "    fcm_dir = config.get('fcm_dir', '')\n",
    "    plot_dir = config.get('plot_dir', '')\n",
    "    \n",
    "    # Filter parameters\n",
    "    filter_params = config.get('filter_params', {})\n",
    "    filter_length_km = filter_params.get('filter_length_km', 10)\n",
    "    filter_time_hours = filter_params.get('filter_time_hours', 1.5)\n",
    "    filter_space = filter_params.get('filter_space', pl.col(\"lenght_km\") < filter_length_km)\n",
    "    filter_time = filter_params.get('filter_time', pl.col(\"time_hours\") < filter_time_hours)\n",
    "    \n",
    "    # Distribution parameters\n",
    "    dist_params = config.get('distribution_params', {})\n",
    "    bin_size_time_hours = dist_params.get('bin_size_time_hours', 0.05)\n",
    "    bin_size_length_km = dist_params.get('bin_size_length_km', 1)\n",
    "    range_time_hours = dist_params.get('range_time_hours', [0.1, filter_time_hours])\n",
    "    range_length_km = dist_params.get('range_length_km', [0.1, filter_length_km])\n",
    "    enriched_vector_length = dist_params.get('enriched_vector_length', 50)\n",
    "    cut_length = dist_params.get('cut_length', 4)\n",
    "    case = dist_params.get('case', \"\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Starting time and length distribution analysis...\")\n",
    "    \n",
    "    # Compute distributions\n",
    "    feature_2_day_2_x, feature_2_day_2_y, feature_2_day_2_y_fit, feature_2_day_2_mean = compute_distributions(\n",
    "        features,\n",
    "        dates,\n",
    "        fcm_dir,\n",
    "        filter_space,\n",
    "        filter_time,\n",
    "        bin_size_time_hours,\n",
    "        bin_size_length_km,\n",
    "        range_time_hours,\n",
    "        range_length_km,\n",
    "        enriched_vector_length\n",
    "    )\n",
    "    \n",
    "    # Plot distributions and get fit parameters\n",
    "    fit_params = plot_distributions(\n",
    "        features,\n",
    "        dates,\n",
    "        feature_2_day_2_x,\n",
    "        feature_2_day_2_y,\n",
    "        feature_2_day_2_mean,\n",
    "        enriched_vector_length,\n",
    "        plot_dir,\n",
    "        range_time_hours,\n",
    "        range_length_km,\n",
    "        cut_length,\n",
    "        case\n",
    "    )\n",
    "    \n",
    "    print(\"Analysis complete. Results saved to:\", plot_dir)\n",
    "    \n",
    "    return fit_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Paper: Do Here (single day separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distribution Analysis with Class Conditioning\n",
    "# ===================================================\n",
    "# \n",
    "# This script performs comprehensive statistical analysis of trajectory features \n",
    "# (time_hours and lenght_km) conditioned on movement classes (0, 1, 2, 3).\n",
    "# It fits exponential and power-law distributions to understand mobility patterns.\n",
    "#\n",
    "# INPUT DATA:\n",
    "# -----------\n",
    "# - FCM files: CSV files containing trajectory data with columns:\n",
    "#   * time_hours: Duration of trajectories in hours\n",
    "#   * lenght_km: Length of trajectories in kilometers  \n",
    "#   * class: Movement class labels (0, 1, 2, 3)\n",
    "# - Date range: 8 specific dates from 2022-2023\n",
    "# - File format: bologna_mdt_{date}_{date}_fcm.csv\n",
    "#\n",
    "# CONFIGURATION PARAMETERS:\n",
    "# -------------------------\n",
    "# filter_lenght_km = 10      # Max trajectory length (km)\n",
    "# filter_time_hours = 1.5    # Max trajectory duration (hours)\n",
    "# bin_size_time_hours = 0.1  # Time histogram bin size\n",
    "# bin_size_length_km = 1     # Length histogram bin size\n",
    "# cut_thresholds = {1: 3, 2: 4}  # Length cutoffs for classes 1,2\n",
    "#\n",
    "# PROCESSING WORKFLOW:\n",
    "# -------------------\n",
    "# 1. DATA LOADING & FILTERING\n",
    "#    - Load FCM data for each date\n",
    "#    - Apply spatial filter: length < 10 km\n",
    "#    - Apply temporal filter: time < 1.5 hours\n",
    "#\n",
    "# 2. FEATURE PROCESSING (Per Date)\n",
    "#    For each feature (time_hours, lenght_km):\n",
    "#    A. Overall Distribution (no class conditioning):\n",
    "#       - Extract all feature values\n",
    "#       - Compute histogram distribution\n",
    "#       - Fit exponential/power-law models\n",
    "#       - Calculate mean values\n",
    "#    \n",
    "#    B. Class-Conditional Distributions:\n",
    "#       For each class (0, 1, 2, 3):\n",
    "#       - Filter data by class label\n",
    "#       - Compute class-specific histograms\n",
    "#       - Apply class-specific length cuts (classes 1,2)\n",
    "#       - Fit statistical models\n",
    "#       - Calculate class means and variances\n",
    "#\n",
    "# 3. VISUALIZATION (Per Date)\n",
    "#    - Plot distributions for all 4 classes on same figure\n",
    "#    - Add fitted curves (exponential/power-law)\n",
    "#    - Include inset plot showing class means\n",
    "#    - Save as: {feature}_distribution_by_classes_{date}.png\n",
    "#\n",
    "# 4. CROSS-DATE AGGREGATION\n",
    "#    - Normalize distributions across all dates\n",
    "#    - Compute weighted averages per class\n",
    "#    - Apply enrichment to standardize vector lengths\n",
    "#    - Refit models to aggregated data\n",
    "#\n",
    "# 5. FINAL VISUALIZATION (Aggregated)\n",
    "#    - Plot average distributions across all dates\n",
    "#    - Show fitted models for each class\n",
    "#    - Include error bars for class means\n",
    "#    - Save as: {feature}_distribution_by_classes_averaged_over_days_concat.png\n",
    "#\n",
    "# OUTPUT FILES:\n",
    "# -------------\n",
    "# PLOTS:\n",
    "# - Daily plots (16 files): {feature}_distribution_by_classes_{date}.png\n",
    "#   Individual date analysis for time_hours and lenght_km\n",
    "# - Aggregated plots (2 files): {feature}_distribution_by_classes_averaged_over_days_concat.png\n",
    "#   Cross-date averaged results\n",
    "#\n",
    "# DATA FILES:\n",
    "# - Fit parameters (4 CSV files):\n",
    "#   * fit_info_final_version_{feature}.csv: Daily fit results\n",
    "#   * fit_info_final_version_{feature}_concat.csv: Aggregated fit results\n",
    "# - Contains: Model parameters, R² values, exponential/power-law coefficients\n",
    "#\n",
    "# KEY ANALYSIS FEATURES:\n",
    "# ----------------------\n",
    "# STATISTICAL MODELING:\n",
    "# - Exponential fits: For length distributions\n",
    "# - Power-law fits: For time distributions  \n",
    "# - Model comparison: Automatic selection based on error metrics\n",
    "#\n",
    "# CLASS-SPECIFIC PROCESSING:\n",
    "# - Dynamic thresholds: Different length cuts per class\n",
    "# - Separate fitting: Independent models per movement class\n",
    "# - Comparative analysis: Cross-class statistical comparison\n",
    "#\n",
    "# ROBUSTNESS:\n",
    "# - Data enrichment: Standardized vector lengths for comparison\n",
    "# - Normalization: Consistent probability distributions\n",
    "# - Error handling: Graceful handling of missing/invalid data\n",
    "#\n",
    "# SCIENTIFIC APPLICATION:\n",
    "# This analysis reveals how different mobility classes (slow/fast, short/long trips) \n",
    "# follow distinct statistical patterns, enabling:\n",
    "# - Urban mobility modeling\n",
    "# - Transportation pattern classification\n",
    "# - Predictive trajectory analysis\n",
    "# - Policy-relevant mobility insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin conditional class Plot distribution length and time\n",
      "Feature: time_hours\n",
      "StrDate: 2022-01-31\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"lenght_km\"; valid columns: [\"id_act;lenght;time;av_speed;v_max;v_min;cnt;av_accel;a_max;class;p;origin_lat;origin_lon;destination_lat;destination_lon;start_time;end_time\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStrDate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Load data for this date\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m fcm_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# df traj to cut\u001b[39;00m\n\u001b[1;32m     77\u001b[0m df_cut_traj \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fcm_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbologna_mdt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cut_traj.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(date_str)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(date_str):\n\u001b[1;32m     49\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fcm_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbologna_mdt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fcm.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geostuff/lib/python3.12/site-packages/polars/dataframe/frame.py:4268\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, *predicates, **constraints)\u001b[0m\n\u001b[1;32m   4168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(\n\u001b[1;32m   4169\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4170\u001b[0m     \u001b[38;5;241m*\u001b[39mpredicates: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstraints: Any,\n\u001b[1;32m   4178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   4179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4180\u001b[0m \u001b[38;5;124;03m    Filter the rows in the DataFrame based on one or more predicate expressions.\u001b[39;00m\n\u001b[1;32m   4181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4266\u001b[0m \u001b[38;5;124;03m    └─────┴─────┴─────┘\u001b[39;00m\n\u001b[1;32m   4267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredicates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geostuff/lib/python3.12/site-packages/polars/lazyframe/frame.py:1967\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, no_optimization, streaming, background, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;66;03m# Only for testing purposes atm.\u001b[39;00m\n\u001b[1;32m   1965\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mColumnNotFoundError\u001b[0m: unable to find column \"lenght_km\"; valid columns: [\"id_act;lenght;time;av_speed;v_max;v_min;cnt;av_accel;a_max;class;p;origin_lat;origin_lon;destination_lat;destination_lon;start_time;end_time\"]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# Configuration parameters\n",
    "StrDates = [\"2022-01-31\", \"2022-07-01\", \"2022-08-05\", \"2022-11-11\", \"2022-12-30\", \"2023-01-01\", \"2022-12-31\", \"2023-03-18\"]\n",
    "labels = [\"0\", \"1\", \"2\", \"3\"]\n",
    "Features = [\"time_hours\", \"lenght_km\"]\n",
    "Classes = [0, 1, 2, 3]\n",
    "fcm_dir = \"/home/aamad/codice/city-pro/output/bologna_mdt_center\"\n",
    "PlotDir = os.path.join(fcm_dir, \"plots\")\n",
    "filter_lenght_km = 10\n",
    "filter_time_hours = 1.5\n",
    "filter_space = pl.col(\"lenght_km\") < filter_lenght_km\n",
    "filter_time = pl.col(\"time_hours\") < filter_time_hours\n",
    "\n",
    "# Fcm,df_traj_cut column names\n",
    "col_user_fcm = \"user_id\"\n",
    "col_user_cut = \"id_act\"\n",
    "\n",
    "\n",
    "# Data distribution parameters\n",
    "bin_size_time_hours = 0.1\n",
    "bin_size_length_km = 1\n",
    "range_time_hours = [0.1, filter_time_hours]\n",
    "range_length_km = [0.1, filter_lenght_km]\n",
    "enriched_vector_length = 50\n",
    "\n",
    "# Class-specific cut thresholds\n",
    "cut_thresholds = {1: 3, 2: 4}\n",
    "\n",
    "# Initialize data structures\n",
    "data_structures = {\n",
    "    \"by_feature_day_class\": init_data_structures(StrDates, Classes, Features),\n",
    "    \"by_feature_day\": init_data_structures(StrDates, None, Features),\n",
    "    \"by_feature_class\": init_data_structures(None, Classes, Features)\n",
    "}\n",
    "\n",
    "# Initialize fit information structures\n",
    "fit_info, fit_info_concat, dict_Lkclass, dict_Lkclass_concat = init_fit_info()\n",
    "\n",
    "print(\"Begin conditional class Plot distribution length and time\")\n",
    "\n",
    "# Create helper function to load data for a specific date\n",
    "def load_data(date_str):\n",
    "    path = os.path.join(fcm_dir, f\"bologna_mdt_{date_str}_{date_str}_fcm.csv\")\n",
    "    return pl.read_csv(path).filter(filter_space, filter_time)\n",
    "\n",
    "# Parameters for processing\n",
    "processing_params = {\n",
    "    \"range_time_hours\": range_time_hours,\n",
    "    \"bin_size_time_hours\": bin_size_time_hours,\n",
    "    \"range_length_km\": range_length_km,\n",
    "    \"bin_size_length_km\": bin_size_length_km,\n",
    "    \"enriched_vector_length\": enriched_vector_length,\n",
    "    \"cut_thresholds\": cut_thresholds\n",
    "}\n",
    "\n",
    "# Process each feature\n",
    "for feature in Features:\n",
    "    print(f\"Feature: {feature}\")\n",
    "    \n",
    "    # Reset fit info for each feature\n",
    "    fit_info, fit_info_concat, dict_Lkclass, dict_Lkclass_concat = init_fit_info()\n",
    "    all_data_combined = None\n",
    "    \n",
    "    # Process each date\n",
    "    for date_str in StrDates:\n",
    "        print(f\"StrDate: {date_str}\")\n",
    "        \n",
    "        # Load data for this date\n",
    "        fcm_data = load_data(date_str)\n",
    "        # df traj to cut\n",
    "        df_cut_traj = pl.read_csv(os.path.join(fcm_dir, f\"bologna_mdt_{date_str}_{date_str}_cut_traj.csv\"))\n",
    "        fcm_data = filter_users_if_one_point_out_of_polygon(fcm_data,df_cut_traj,col_user_fcm,col_user_cut)\n",
    "        # Combine data for concatenated analysis\n",
    "        if all_data_combined is None:\n",
    "            all_data_combined = fcm_data\n",
    "        else:\n",
    "            all_data_combined = pl.concat([all_data_combined, fcm_data])\n",
    "        \n",
    "        # Process non-class specific data (entire dataset)\n",
    "        feature_data = fcm_data[feature].to_list()\n",
    "        x, n, A_fit, b_fit, y_fit, is_exp, x_mean = process_single_fit(\n",
    "            feature_data, \n",
    "            feature,\n",
    "            range_time_hours,\n",
    "            bin_size_time_hours,\n",
    "            range_length_km,\n",
    "            bin_size_length_km,\n",
    "            enriched_vector_length\n",
    "        )\n",
    "        \n",
    "        # Update fit info\n",
    "        fit_info_concat, dict_Lkclass_concat = update_fit_info(\n",
    "            fit_info_concat,\n",
    "            dict_Lkclass_concat,\n",
    "            100,  # Special class for overall data\n",
    "            x_mean,\n",
    "            b_fit,\n",
    "            is_exp,\n",
    "            date_str\n",
    "        )\n",
    "        \n",
    "        print(f\"size x: {len(x)}, size n: {len(n)}, size y_fit: {len(y_fit)}, <x>: {x_mean}\")\n",
    "        \n",
    "        # Store results in data structures\n",
    "        data_structures[\"by_feature_day\"][\"x\"][feature][date_str] = x\n",
    "        data_structures[\"by_feature_day\"][\"y\"][feature][date_str] = n\n",
    "        data_structures[\"by_feature_day\"][\"y_fit\"][feature][date_str] = y_fit\n",
    "        data_structures[\"by_feature_day\"][\"y_avg\"][feature][date_str] = x_mean\n",
    "        \n",
    "        # Process class-specific data\n",
    "        x_means_day = np.zeros(len(Classes))\n",
    "        x_variance_day = np.zeros(len(Classes))\n",
    "        \n",
    "        # Process each class\n",
    "        for class_idx in Classes:\n",
    "            print(f\"Class: {class_idx}\")\n",
    "            \n",
    "            # Filter data for this class\n",
    "            class_data = fcm_data.filter(pl.col(\"class\") == class_idx)[feature].to_list()\n",
    "            \n",
    "            # Process data with class-specific settings\n",
    "            class_params = processing_params.copy()\n",
    "            class_params[\"class_idx\"] = class_idx\n",
    "            \n",
    "            x, n, A_fit, b_fit, y_fit, is_exp, x_mean = process_single_fit(\n",
    "                class_data, \n",
    "                feature,\n",
    "                **class_params\n",
    "            )\n",
    "            \n",
    "            # Update fit info\n",
    "            fit_info, dict_Lkclass = update_fit_info(\n",
    "                fit_info,\n",
    "                dict_Lkclass,\n",
    "                class_idx,\n",
    "                x_mean,\n",
    "                b_fit,\n",
    "                is_exp,\n",
    "                date_str\n",
    "            )\n",
    "            \n",
    "            # Store results in data structures\n",
    "            data_structures[\"by_feature_day_class\"][\"x\"][feature][date_str][class_idx] = x\n",
    "            data_structures[\"by_feature_day_class\"][\"y\"][feature][date_str][class_idx] = n\n",
    "            data_structures[\"by_feature_day_class\"][\"y_fit\"][feature][date_str][class_idx] = y_fit\n",
    "            data_structures[\"by_feature_day_class\"][\"y_avg\"][feature][date_str][class_idx] = x_mean\n",
    "            \n",
    "            # Calculate and store statistics\n",
    "            x_means_day[class_idx] = x_mean\n",
    "            x_variance_day[class_idx] = np.nanstd(class_data)\n",
    "        \n",
    "        # Create visualization for this day and feature\n",
    "        print(f\"Plot the distribution of the feature conditioned on the class {date_str}\")\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        \n",
    "        # Define x-limits based on feature\n",
    "        x_lim_min = range_time_hours[0] if feature == \"time_hours\" else range_length_km[0]\n",
    "        x_lim_max = range_time_hours[1] if feature == \"time_hours\" else range_length_km[1]\n",
    "        \n",
    "        # Plot each class\n",
    "        for class_idx in Classes:\n",
    "            ax = plot_distribution(\n",
    "                ax,\n",
    "                class_idx,\n",
    "                data_structures[\"by_feature_day_class\"][\"x\"][feature][date_str][class_idx],\n",
    "                data_structures[\"by_feature_day_class\"][\"y\"][feature][date_str][class_idx],\n",
    "                data_structures[\"by_feature_day_class\"][\"y_fit\"][feature][date_str][class_idx],\n",
    "                labels,\n",
    "                feature,\n",
    "                x_lim_min,\n",
    "                x_lim_max\n",
    "            )\n",
    "        \n",
    "        ax.legend()\n",
    "        \n",
    "        # Add inset plot showing mean values\n",
    "        ax_inset = add_inset_plot(\n",
    "            fig,\n",
    "            ax,\n",
    "            Classes,\n",
    "            {feature: {date_str: x_means_day}},\n",
    "            [date_str],\n",
    "            feature\n",
    "        )\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(PlotDir, f\"{feature}_distribution_by_classes_{date_str}.png\"))\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Save fit information to CSV\n",
    "    pl.DataFrame(fit_info).write_csv(os.path.join(PlotDir, f\"fit_info_final_version_{feature}.csv\"))\n",
    "    \n",
    "    # Process concatenated data across dates for each class\n",
    "    print(\"Processing concatenated data\")\n",
    "    \n",
    "    # Use last date's class 0 x-values as reference\n",
    "    ref_x = np.array(data_structures[\"by_feature_day_class\"][\"x\"][feature][StrDates[-1]][0])\n",
    "    size_x = len(ref_x)\n",
    "    \n",
    "    # Initialize arrays for concatenated results\n",
    "    x_means_concat = np.zeros(len(Classes))\n",
    "    x_variance_concat = np.zeros(len(Classes))\n",
    "    \n",
    "    # Initialize concatenated y values\n",
    "    for class_idx in Classes:\n",
    "        data_structures[\"by_feature_class\"][\"y\"][feature][class_idx] = np.zeros(size_x)\n",
    "    \n",
    "    # First, aggregate normalized distributions across dates\n",
    "    for date_str in StrDates:\n",
    "        for class_idx in Classes:\n",
    "            # Get distribution for this class/date and normalize\n",
    "            n = np.array(data_structures[\"by_feature_day_class\"][\"y\"][feature][date_str][class_idx])\n",
    "            n_normalized = n / np.sum(n) if np.sum(n) > 0 else n\n",
    "            \n",
    "            # Ensure vector length matches\n",
    "            n_normalized = enrich_vector_to_length(n_normalized, size_x)\n",
    "            \n",
    "            # Add to aggregated distribution with equal weight per day\n",
    "            data_structures[\"by_feature_class\"][\"y\"][feature][class_idx] += (n_normalized / len(StrDates))\n",
    "    \n",
    "    # Process aggregated distributions\n",
    "    for class_idx in Classes:\n",
    "        # Get aggregated distribution\n",
    "        y_agg = data_structures[\"by_feature_class\"][\"y\"][feature][class_idx]\n",
    "        \n",
    "        # Apply class-specific cuts if needed\n",
    "        if \"len\" in feature:\n",
    "            if class_idx == 1:\n",
    "                y_class = y_agg[ref_x > cut_thresholds[1]]\n",
    "                x_fit = ref_x[ref_x > cut_thresholds[1]]\n",
    "            elif class_idx == 2:\n",
    "                y_class = y_agg[ref_x > cut_thresholds[2]]\n",
    "                x_fit = ref_x[ref_x > cut_thresholds[2]]\n",
    "            else:\n",
    "                y_class = y_agg\n",
    "                x_fit = ref_x\n",
    "        else:\n",
    "            y_class = y_agg\n",
    "            x_fit = ref_x\n",
    "        \n",
    "        # Fit the distribution\n",
    "        x_result, n_result, A_fit, b_fit, y_fit, is_exp = compute_single_fit_from_xy(\n",
    "            x_fit,\n",
    "            y_class,\n",
    "            feature,\n",
    "            bin_size_time_hours,\n",
    "            bin_size_length_km,\n",
    "            range_time_hours,\n",
    "            range_length_km,\n",
    "            enriched_vector_length,\n",
    "            class_idx\n",
    "        )\n",
    "        \n",
    "        # Normalize\n",
    "        n_result = n_result / np.sum(n_result) if np.sum(n_result) > 0 else n_result\n",
    "        \n",
    "        # Calculate statistics\n",
    "        x_mean = np.sum(n_result * ref_x) if len(n_result) == len(ref_x) else np.mean(n_result * x_result)\n",
    "        x_variance = np.sqrt(np.sum(n_result * (ref_x - x_mean)**2) / len(n_result)) if len(n_result) == len(ref_x) else 0\n",
    "        \n",
    "        # Update fit info\n",
    "        fit_info_concat, dict_Lkclass_concat = update_fit_info(\n",
    "            fit_info_concat,\n",
    "            dict_Lkclass_concat,\n",
    "            class_idx,\n",
    "            x_mean,\n",
    "            b_fit,\n",
    "            is_exp,\n",
    "            \"concatenated\"  # Use special date identifier for concatenated data\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        data_structures[\"by_feature_class\"][\"x\"][feature][class_idx] = ref_x\n",
    "        data_structures[\"by_feature_class\"][\"y\"][feature][class_idx] = n_result\n",
    "        data_structures[\"by_feature_class\"][\"y_fit\"][feature][class_idx] = y_fit\n",
    "        data_structures[\"by_feature_class\"][\"y_avg\"][feature][class_idx] = x_mean\n",
    "        \n",
    "        # Store statistics\n",
    "        x_means_concat[class_idx] = x_mean\n",
    "        x_variance_concat[class_idx] = x_variance\n",
    "    \n",
    "    # Plot concatenated distributions\n",
    "    print(\"Plot the distribution of the feature conditioned on the class (concatenated)\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    \n",
    "    # Plot each class\n",
    "    for class_idx in Classes:\n",
    "        ax = plot_distribution(\n",
    "            ax,\n",
    "            class_idx,\n",
    "            data_structures[\"by_feature_class\"][\"x\"][feature][class_idx],\n",
    "            data_structures[\"by_feature_class\"][\"y\"][feature][class_idx],\n",
    "            data_structures[\"by_feature_class\"][\"y_fit\"][feature][class_idx],\n",
    "            labels,\n",
    "            feature,\n",
    "            x_lim_min,\n",
    "            x_lim_max\n",
    "        )\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    # Add inset plot\n",
    "    ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=\"upper center\")\n",
    "    ax_inset.errorbar(\n",
    "        Classes,\n",
    "        x_means_concat,\n",
    "        yerr=x_variance_concat,\n",
    "        fmt='o',\n",
    "        color='blue',\n",
    "        ecolor='red',\n",
    "        elinewidth=1,\n",
    "        capsize=3,\n",
    "    )\n",
    "    \n",
    "    ax_inset.set_xticks(Classes)\n",
    "    if feature == \"time_hours\":\n",
    "        ax_inset.set_xlabel(\"class\", fontsize=12)\n",
    "        ax_inset.set_ylabel(r\"$\\langle t \\rangle$ (h)\", fontsize=12)\n",
    "    else:\n",
    "        ax_inset.set_xlabel(\"class\", fontsize=12)\n",
    "        ax_inset.set_ylabel(r\"$\\langle l \\rangle$ (km)\", fontsize=12)\n",
    "    \n",
    "    # Save figure for concatenated data\n",
    "    plt.savefig(os.path.join(PlotDir, f\"{feature}_distribution_by_classes_averaged_over_days_concat.png\"))\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Save fit information to CSV\n",
    "    pl.DataFrame(fit_info_concat).write_csv(os.path.join(PlotDir, f\"fit_info_final_version_{feature}_concat.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Averaged all days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Distribution Analysis\n",
    "# ============================\n",
    "#\n",
    "# Computes average distributions across all dates for each feature,\n",
    "# fits appropriate statistical models, and creates publication-ready plots.\n",
    "#\n",
    "# KEY FEATURES:\n",
    "# - Cross-date averaging with proper normalization\n",
    "# - Forced model selection: Power law for time, Exponential for length\n",
    "# - Smoothing with moving average windows\n",
    "# - Plotting with logarithmic scales\n",
    "#\n",
    "# OUTPUT:\n",
    "# - {feature}_average_distribution{case}.png\n",
    "# - average_distribution_fit_params{case}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'features': [\"time_hours\", \"lenght_km\"],\n",
    "    'dates': [\"2022-01-31\", \"2022-07-01\", \"2022-08-05\", \"2022-11-11\", \n",
    "             \"2022-12-30\", \"2023-01-01\", \"2022-12-31\", \"2023-03-18\"],\n",
    "    'fcm_dir': \"/home/aamad/codice/city-pro/output/bologna_mdt_center\",\n",
    "    'plot_dir': \"/home/aamad/codice/city-pro/output/bologna_mdt_center/plots\",\n",
    "    \n",
    "    'filter_params': {\n",
    "        'filter_length_km': 10,\n",
    "        'filter_time_hours': 1.5,\n",
    "        'filter_space': pl.col(\"lenght_km\") < 10,\n",
    "        'filter_time': pl.col(\"time_hours\") < 1.5,\n",
    "    },\n",
    "    \n",
    "    'distribution_params': {\n",
    "        'bin_size_time_hours': 0.05,\n",
    "        'bin_size_length_km': 1,\n",
    "        'range_time_hours': [0.1, 1.5],\n",
    "        'range_length_km': [0.1, 10],\n",
    "        'enriched_vector_length': 50,\n",
    "        'cut_length': 4,\n",
    "        'window_size': 3,\n",
    "        'case': \"_forced_fits\"  # Add a suffix to distinguish these from other plots\n",
    "    }\n",
    "}\n",
    "run_analysis(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Polished ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_plot_average_distributions(\n",
    "    features: List[str],\n",
    "    dates: List[str],\n",
    "    fcm_dir: str,\n",
    "    plot_dir: str,\n",
    "    filter_space: pl.Expr,\n",
    "    filter_time: pl.Expr,\n",
    "    bin_size_time_hours: float,\n",
    "    bin_size_length_km: float,\n",
    "    range_time_hours: List[float],\n",
    "    range_length_km: List[float],\n",
    "    enriched_vector_length: int = 50,\n",
    "    cut_length: float = 4,\n",
    "    window_size: int = 3,\n",
    "    case: str = \"\"\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Calculate the average distribution over all days for each feature, fit appropriate models, and plot.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize dictionaries to store data\n",
    "    daily_distributions = {feature: {} for feature in features}\n",
    "    feature_means = {feature: {} for feature in features}\n",
    "    \n",
    "    # Initialize result dictionaries for fits\n",
    "    fit_params = {feature: {} for feature in features}\n",
    "    \n",
    "    # Step 1: Extract data and compute distribution for each day and feature\n",
    "    for feature in features:\n",
    "        print(f\"Processing feature: {feature}\")\n",
    "        \n",
    "        # Step 1a: For each day, extract data and compute distribution\n",
    "        for date_str in dates:\n",
    "            print(f\"  Processing date: {date_str}\")\n",
    "            \n",
    "            # Load data for this day\n",
    "            fcm_data = load_data(fcm_dir, date_str, filter_space, filter_time)\n",
    "            if fcm_data is None or fcm_data.height == 0:\n",
    "                print(f\"  No data for {date_str}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Extract feature data\n",
    "            feature_data = fcm_data[feature].to_numpy()\n",
    "            if len(feature_data) == 0:\n",
    "                print(f\"  No {feature} data for {date_str}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Compute mean for this day\n",
    "            feature_means[feature][date_str] = np.nanmean(feature_data)\n",
    "            \n",
    "            # Compute distribution for this day\n",
    "            try:\n",
    "                bins, bin_range = from_feature_to_nin_bin_range(\n",
    "                    feature, \n",
    "                    range_time_hours, \n",
    "                    bin_size_time_hours, \n",
    "                    range_length_km, \n",
    "                    bin_size_length_km\n",
    "                )\n",
    "                \n",
    "                x, n = from_data_to_cut_distribution(feature_data, bins, bin_range)\n",
    "                \n",
    "                # Store the distribution for this day\n",
    "                daily_distributions[feature][date_str] = {\n",
    "                    'x': x,\n",
    "                    'n': n\n",
    "                }\n",
    "                print(f\"  Distribution for {feature} on {date_str}: x shape = {len(x)}, n shape = {len(n)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error computing distribution for {date_str}, feature {feature}: {e}\")\n",
    "    \n",
    "    # Step 2: Compute average distribution over all days for each feature\n",
    "    average_distributions = {}\n",
    "    for feature in features:\n",
    "        print(f\"Computing average distribution for {feature}\")\n",
    "        \n",
    "        # Create a reference x-axis for this feature with consistent points\n",
    "        if feature == 'time_hours':\n",
    "            ref_x = np.linspace(range_time_hours[0], range_time_hours[1], enriched_vector_length)\n",
    "        else:  # lenght_km\n",
    "            ref_x = np.linspace(range_length_km[0], range_length_km[1], enriched_vector_length)\n",
    "        \n",
    "        # Initialize arrays for summing distributions\n",
    "        summed_n = np.zeros(enriched_vector_length)\n",
    "        valid_days = 0\n",
    "        \n",
    "        # Process each day\n",
    "        for date_str, dist in daily_distributions[feature].items():\n",
    "            if len(dist['x']) == 0 or len(dist['n']) == 0:\n",
    "                print(f\"  Empty distribution for {feature} on {date_str}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Check if x and n have the same length\n",
    "            if len(dist['x']) != len(dist['n']):\n",
    "                print(f\"  Warning: x and n have different lengths for {feature} on {date_str}\")\n",
    "                max_len = max(len(dist['x']), len(dist['n']))\n",
    "                print(f\"  enrich {max_len}\")\n",
    "                x_day = enrich_vector_to_length(dist['x'], max_len)\n",
    "                n_day = enrich_vector_to_length(dist['n'], max_len)\n",
    "            else:\n",
    "                x_day = dist['x']\n",
    "                n_day = dist['n']\n",
    "            \n",
    "            # Handle case where x values might not be sorted (required for np.interp)\n",
    "            sort_idx = np.argsort(x_day)\n",
    "            x_day = np.array(x_day)[sort_idx]\n",
    "            n_day = np.array(n_day)[sort_idx]\n",
    "            \n",
    "            # Resample the distribution to match ref_x\n",
    "            try:\n",
    "                resampled_n = np.interp(\n",
    "                    ref_x, \n",
    "                    x_day, \n",
    "                    n_day,\n",
    "                    left=0,\n",
    "                    right=0\n",
    "                )\n",
    "                \n",
    "                # Normalize the distribution\n",
    "                if np.sum(resampled_n) > 0:\n",
    "                    resampled_n = resampled_n / np.sum(resampled_n)\n",
    "                \n",
    "                # Add to sum\n",
    "                summed_n += resampled_n\n",
    "                valid_days += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error resampling distribution for {feature} on {date_str}: {e}\")\n",
    "                print(f\"  x_day shape: {x_day.shape}, n_day shape: {n_day.shape}\")\n",
    "        \n",
    "        if valid_days == 0:\n",
    "            print(f\"  No valid distributions for {feature}\")\n",
    "            continue\n",
    "            \n",
    "        # Compute average\n",
    "        avg_n = summed_n / valid_days\n",
    "        \n",
    "        # Smooth the distribution with a moving average\n",
    "        avg_n_smooth = np.zeros_like(avg_n)\n",
    "        for i in range(len(avg_n)):\n",
    "            start = max(0, i - window_size // 2)\n",
    "            end = min(len(avg_n), i + window_size // 2 + 1)\n",
    "            avg_n_smooth[i] = np.mean(avg_n[start:end])\n",
    "            \n",
    "        # Store average distribution\n",
    "        average_distributions[feature] = {\n",
    "            'x': ref_x,\n",
    "            'n': avg_n_smooth,\n",
    "            'mean': np.mean([mean for mean in feature_means[feature].values()])\n",
    "        }\n",
    "    \n",
    "    # Step 3: Fit models to the average distributions\n",
    "    for feature in features:\n",
    "        if feature not in average_distributions:\n",
    "            continue\n",
    "            \n",
    "        x = average_distributions[feature]['x']\n",
    "        n = average_distributions[feature]['n']\n",
    "        \n",
    "        if feature == 'time_hours':\n",
    "            # Fit power law for time_hours\n",
    "            try:\n",
    "                # Workaround for zeros in power law fitting\n",
    "                # Find non-zero values for proper power law fitting\n",
    "                mask_nonzero = np.logical_and(x > 0, n > 0)\n",
    "                if np.sum(mask_nonzero) < 3:  # Need at least 3 points for a decent fit\n",
    "                    print(f\"  Not enough non-zero data points for power law fit of {feature}\")\n",
    "                    # Apply small epsilon to zeros\n",
    "                    x_pl = x.copy()\n",
    "                    n_pl = n.copy()\n",
    "                    n_pl[n_pl <= 0] = np.min(n_pl[n_pl > 0]) * 0.1  # Small fraction of smallest non-zero value\n",
    "                else:\n",
    "                    x_pl = x[mask_nonzero]\n",
    "                    n_pl = n[mask_nonzero]\n",
    "                \n",
    "                # Fit both models\n",
    "                A_exp, beta_exp, exp_, error_exp, R2_exp, A_pl, alpha_pl, pl_, error_pl, R2_pl, bins_plot = \\\n",
    "                    compare_exponential_power_law_from_xy(x_pl, n_pl)\n",
    "                \n",
    "                # For time_hours, always use power law regardless of error\n",
    "                # But extend the fit to the full range\n",
    "                full_pl = A_pl * np.power(x, alpha_pl)  # Calculate over the full range\n",
    "                \n",
    "                fit_params[feature] = {\n",
    "                    'model': 'power_law',\n",
    "                    'A': A_pl,\n",
    "                    'alpha': alpha_pl,\n",
    "                    'R2': R2_pl,\n",
    "                    'fitted_y': full_pl,\n",
    "                    'fitted_range': x_pl  # Store the range that was actually used for fitting\n",
    "                }\n",
    "                print(f\"  Fit power law to {feature}: alpha = {alpha_pl:.4f}, R² = {R2_pl:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error fitting power law to {feature}: {e}\")\n",
    "                fit_params[feature] = {\n",
    "                    'model': 'none',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        elif 'lenght' in feature:\n",
    "            # Fit exponential only up to cut_length for lenght_km\n",
    "            try:\n",
    "                # Apply cut length\n",
    "                mask = x <= cut_length\n",
    "                x_masked = x[mask]\n",
    "                n_masked = n[mask]\n",
    "                \n",
    "                if len(x_masked) == 0:\n",
    "                    print(f\"  No data points below cut_length={cut_length} for {feature}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Fit exponential up to cut length\n",
    "                A_exp, beta_exp, exp_, error_exp, R2_exp, bins_plot = fit_expo(x_masked, n_masked)\n",
    "                \n",
    "                # Create full-range fitted curve for plotting (but fit is only valid up to cut_length)\n",
    "                full_exp = A_exp * np.exp(beta_exp * x)\n",
    "                \n",
    "                fit_params[feature] = {\n",
    "                    'model': 'exponential',\n",
    "                    'A': A_exp,\n",
    "                    'beta': beta_exp,\n",
    "                    'R2': R2_exp,\n",
    "                    'fitted_y': full_exp,\n",
    "                    'cut_length': cut_length\n",
    "                }\n",
    "                print(f\"  Fit exponential to {feature}: beta = {beta_exp:.4f}, R² = {R2_exp:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error fitting exponential to {feature}: {e}\")\n",
    "                fit_params[feature] = {\n",
    "                    'model': 'none',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "    \n",
    "    # Step 4: Plot the average distributions with fits\n",
    "    for feature, dist in average_distributions.items():\n",
    "        print(f\"Plotting {feature} distribution\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Plot the average distribution\n",
    "        ax.scatter(dist['x'], dist['n'], \n",
    "                  label=f\"Average Distribution\", \n",
    "                  alpha=0.7, color='black', s=30)\n",
    "        main_labels = [\"average\"]\n",
    "        # Plot individual day distributions with lower opacity\n",
    "        for date_str, day_dist in daily_distributions[feature].items():\n",
    "            if len(day_dist['x']) == 0 or len(day_dist['n']) == 0:\n",
    "                continue\n",
    "            main_labels.append(date_str)\n",
    "    \n",
    "            # Check if x and n have the same length\n",
    "            if len(day_dist['x']) != len(day_dist['n']):\n",
    "                max_len = max(len(day_dist['x']), len(day_dist['n']))\n",
    "                x_day = enrich_vector_to_length(day_dist['x'],max_len)\n",
    "                n_day = enrich_vector_to_length(day_dist['n'],max_len)\n",
    "            else:\n",
    "                x_day = day_dist['x']\n",
    "                n_day = day_dist['n']\n",
    "            \n",
    "            # Handle case where x values might not be sorted (required for np.interp)\n",
    "            sort_idx = np.argsort(x_day)\n",
    "            x_day = np.array(x_day)[sort_idx]\n",
    "            n_day = np.array(n_day)[sort_idx]\n",
    "            \n",
    "            try:\n",
    "                # Resample to match ref_x\n",
    "                resampled_n = np.interp(\n",
    "                    dist['x'], \n",
    "                    x_day, \n",
    "                    n_day,\n",
    "                    left=0,\n",
    "                    right=0\n",
    "                )\n",
    "                \n",
    "                # Normalize\n",
    "                if np.sum(resampled_n) > 0:\n",
    "                    resampled_n = resampled_n / np.sum(resampled_n)\n",
    "                \n",
    "                ax.scatter(dist['x'], resampled_n, \n",
    "                          label=date_str, \n",
    "                          alpha=0.3, s=10)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error plotting day distribution for {feature} on {date_str}: {e}\")\n",
    "        \n",
    "        # Plot the fit if available\n",
    "        if feature in fit_params and fit_params[feature].get('model') != 'none':\n",
    "            fit_info = fit_params[feature]\n",
    "            \n",
    "            if fit_info['model'] == 'power_law':\n",
    "                # Normalize for plotting\n",
    "                y_fit = fit_info['fitted_y']\n",
    "                if np.sum(y_fit) > 0:\n",
    "                    y_fit = y_fit / np.sum(y_fit)\n",
    "                \n",
    "                # Highlight the actual fitted range\n",
    "                ax.plot(dist['x'], y_fit, '--', \n",
    "                       linewidth=2, color='blue',\n",
    "                       label=f\"Power Law Fit (α = {fit_info['alpha']:.4f})\")\n",
    "                \n",
    "                # Add annotation about power law fit if we had to handle zeros\n",
    "                if 'fitted_range' in fit_info and len(fit_info['fitted_range']) < len(dist['x']):\n",
    "                    ax.text(0.05, 0.05, \"Note: Power law fit excludes zero values\", \n",
    "                           transform=ax.transAxes, fontsize=10, alpha=0.7)\n",
    "                \n",
    "            elif fit_info['model'] == 'exponential':\n",
    "                # Normalize for plotting\n",
    "                y_fit = fit_info['fitted_y']\n",
    "                if np.sum(y_fit) > 0:\n",
    "                    y_fit = y_fit / np.sum(y_fit)\n",
    "                \n",
    "                # Get the index corresponding to the cut length\n",
    "                cut_idx = np.searchsorted(dist['x'], cut_length)\n",
    "                \n",
    "                # Plot the fit differently before and after the cut length\n",
    "                ax.plot(dist['x'][:cut_idx+1], y_fit[:cut_idx+1], '-', \n",
    "                       linewidth=3, color='red',\n",
    "                       label=f\"Exponential Fit (β = {fit_info['beta']:.4f})\")\n",
    "                # Add vertical line at cut_length\n",
    "                ax.axvline(x=cut_length, color='gray', linestyle='--', alpha=0.7)\n",
    "                ax.text(cut_length*1.05, ax.get_ylim()[1]*0.8, f'cut = {cut_length} km', \n",
    "                       rotation=90, color='gray')\n",
    "        \n",
    "        # Configure plot\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        if feature == 'time_hours':\n",
    "            ax.set_xlabel('t(h)', fontsize=14)\n",
    "            ax.set_ylabel('P(t)', fontsize=14)\n",
    "            ax.set_xlim(range_time_hours[0], range_time_hours[1])\n",
    "        else:\n",
    "            ax.set_xlabel('l(km)', fontsize=14)\n",
    "            ax.set_ylabel('P(l)', fontsize=14)\n",
    "            ax.set_xlim(range_length_km[0], range_length_km[1])\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        # Get the current data range\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "\n",
    "        # Create exactly 3 logarithmically spaced ticks\n",
    "        log_ymin, log_ymax = np.log10(ymin), np.log10(ymax)\n",
    "        log_ticks = np.linspace(log_ymin, log_ymax, 3)  # 3 points evenly spaced in log space\n",
    "        tick_positions = 10**log_ticks\n",
    "\n",
    "        # Set these ticks directly\n",
    "        ax.set_yticks(tick_positions)\n",
    "\n",
    "        # Create a formatter that displays the exponent\n",
    "        def log_formatter(y, pos):\n",
    "            exponent = np.log10(y)\n",
    "            # Round to 1 decimal place if not close to an integer\n",
    "            if abs(exponent - round(exponent)) < 0.05:\n",
    "                return f'$10^{{{int(round(exponent))}}}$'\n",
    "            else:\n",
    "                return f'$10^{{{exponent:.1f}}}$'\n",
    "\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(log_formatter))\n",
    "\n",
    "        # Add minor ticks between the major ticks for better visualization\n",
    "        minor_locator = ticker.LogLocator(base=10.0, subs=np.arange(2, 10) * 0.1)\n",
    "        ax.yaxis.set_minor_locator(minor_locator)        # Add inset with mean values\n",
    "#        ax_inset = inset_axes(ax, width=\"30%\", height=\"25%\", loc=\"upper right\")\n",
    "#        ax_inset = plot_inset_means(\n",
    "#            feature_means[feature], \n",
    "#            dates,\n",
    "#            feature, \n",
    "#            ax_inset\n",
    "#        )\n",
    "        \n",
    "        # Keep only the first few legend items to avoid overcrowding\n",
    "        ax.legend(main_labels, \n",
    "                 loc='upper right', bbox_to_anchor=(0.98, 0.98),\n",
    "                 fontsize=12, framealpha=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_dir, f\"{feature}_average_distribution{case}.png\"), dpi=300)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Save fit parameters\n",
    "    fit_params_for_json = {\n",
    "        feature: {\n",
    "            'model': params.get('model', 'none'),\n",
    "            'parameters': {\n",
    "                'A': float(params.get('A', 0)),\n",
    "                'alpha' if params.get('model') == 'power_law' else 'beta': \n",
    "                    float(params.get('alpha' if params.get('model') == 'power_law' else 'beta', 0))\n",
    "            },\n",
    "            'R2': float(params.get('R2', 0)),\n",
    "            'cut_length': float(params.get('cut_length', 0)) if params.get('model') == 'exponential' else None\n",
    "        }\n",
    "        for feature, params in fit_params.items()\n",
    "        if params.get('model') != 'none'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(plot_dir, f\"average_distribution_fit_params{case}.json\"), \"w\") as f:\n",
    "            json.dump(fit_params_for_json, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving fit parameters: {e}\")\n",
    "    \n",
    "    return fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_average_distribution_analysis(config: Dict[str, Any]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Run the average distribution analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing configuration parameters\n",
    "            - features: List of features to analyze\n",
    "            - dates: List of date strings\n",
    "            - fcm_dir: Directory containing FCM data files\n",
    "            - plot_dir: Directory to save plots\n",
    "            - filter_params: Dictionary of filter parameters\n",
    "            - distribution_params: Dictionary of distribution parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of fit parameters for each feature\n",
    "    \"\"\"\n",
    "    # Extract configuration parameters\n",
    "    features = config.get('features', [\"time_hours\", \"lenght_km\"])\n",
    "    dates = config.get('dates', [])\n",
    "    fcm_dir = config.get('fcm_dir', '')\n",
    "    plot_dir = config.get('plot_dir', '')\n",
    "    \n",
    "    # Filter parameters\n",
    "    filter_params = config.get('filter_params', {})\n",
    "    filter_length_km = filter_params.get('filter_length_km', 10)\n",
    "    filter_time_hours = filter_params.get('filter_time_hours', 1.5)\n",
    "    filter_space = filter_params.get('filter_space', pl.col(\"lenght_km\") < filter_length_km)\n",
    "    filter_time = filter_params.get('filter_time', pl.col(\"time_hours\") < filter_time_hours)\n",
    "    \n",
    "    # Distribution parameters\n",
    "    dist_params = config.get('distribution_params', {})\n",
    "    bin_size_time_hours = dist_params.get('bin_size_time_hours', 0.05)\n",
    "    bin_size_length_km = dist_params.get('bin_size_length_km', 1)\n",
    "    range_time_hours = dist_params.get('range_time_hours', [0.1, filter_time_hours])\n",
    "    range_length_km = dist_params.get('range_length_km', [0.1, filter_length_km])\n",
    "    enriched_vector_length = dist_params.get('enriched_vector_length', 50)\n",
    "    cut_length = dist_params.get('cut_length', 4)\n",
    "    window_size = dist_params.get('window_size', 3)\n",
    "    case = dist_params.get('case', \"\")\n",
    "    \n",
    "    print(\"Starting average distribution analysis...\")\n",
    "    \n",
    "    # Calculate and plot average distributions\n",
    "    fit_params = calculate_and_plot_average_distributions(\n",
    "        features,\n",
    "        dates,\n",
    "        fcm_dir,\n",
    "        plot_dir,\n",
    "        filter_space,\n",
    "        filter_time,\n",
    "        bin_size_time_hours,\n",
    "        bin_size_length_km,\n",
    "        range_time_hours,\n",
    "        range_length_km,\n",
    "        enriched_vector_length,\n",
    "        cut_length,\n",
    "        window_size,\n",
    "        case\n",
    "    )\n",
    "    \n",
    "    print(\"Analysis complete. Results saved to:\", plot_dir)\n",
    "    \n",
    "    return fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting average distribution analysis...\n",
      "Processing feature: time_hours\n",
      "  Processing date: 2022-01-31\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2022-01-31: x shape = 28, n shape = 27\n",
      "  Processing date: 2022-07-01\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2022-07-01: x shape = 28, n shape = 27\n",
      "  Processing date: 2022-08-05\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2022-08-05: x shape = 28, n shape = 27\n",
      "  Processing date: 2022-11-11\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2022-11-11: x shape = 28, n shape = 27\n",
      "  Processing date: 2022-12-30\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2022-12-30: x shape = 28, n shape = 27\n",
      "  Processing date: 2023-01-01\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2023-01-01: x shape = 28, n shape = 27\n",
      "  Processing date: 2022-12-31\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2022-12-31: x shape = 28, n shape = 27\n",
      "  Processing date: 2023-03-18\n",
      "bin_size:  0.05\n",
      "  Distribution for time_hours on 2023-03-18: x shape = 28, n shape = 27\n",
      "Processing feature: lenght_km\n",
      "  Processing date: 2022-01-31\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2022-01-31: x shape = 10, n shape = 9\n",
      "  Processing date: 2022-07-01\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2022-07-01: x shape = 10, n shape = 9\n",
      "  Processing date: 2022-08-05\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2022-08-05: x shape = 10, n shape = 9\n",
      "  Processing date: 2022-11-11\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2022-11-11: x shape = 10, n shape = 9\n",
      "  Processing date: 2022-12-30\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2022-12-30: x shape = 10, n shape = 9\n",
      "  Processing date: 2023-01-01\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2023-01-01: x shape = 10, n shape = 9\n",
      "  Processing date: 2022-12-31\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2022-12-31: x shape = 10, n shape = 9\n",
      "  Processing date: 2023-03-18\n",
      "bin_size:  1\n",
      "  Distribution for lenght_km on 2023-03-18: x shape = 10, n shape = 9\n",
      "Computing average distribution for time_hours\n",
      "  Warning: x and n have different lengths for time_hours on 2022-01-31\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2022-07-01\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2022-08-05\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2022-11-11\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2022-12-30\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2023-01-01\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2022-12-31\n",
      "  enrich 28\n",
      "  Warning: x and n have different lengths for time_hours on 2023-03-18\n",
      "  enrich 28\n",
      "Computing average distribution for lenght_km\n",
      "  Warning: x and n have different lengths for lenght_km on 2022-01-31\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2022-07-01\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2022-08-05\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2022-11-11\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2022-12-30\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2023-01-01\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2022-12-31\n",
      "  enrich 10\n",
      "  Warning: x and n have different lengths for lenght_km on 2023-03-18\n",
      "  enrich 10\n",
      "  Fit power law to time_hours: alpha = -1.3632, R² = 0.8540\n",
      "  Fit exponential to lenght_km: beta = -0.2555, R² = 0.5524\n",
      "Plotting time_hours distribution\n",
      "Plotting lenght_km distribution\n",
      "Analysis complete. Results saved to: /home/aamad/codice/city-pro/output/bologna_mdt_center/plots\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'features': [\"time_hours\", \"lenght_km\"],\n",
    "    'dates': [\"2022-01-31\", \"2022-07-01\", \"2022-08-05\", \"2022-11-11\", \n",
    "             \"2022-12-30\", \"2023-01-01\", \"2022-12-31\", \"2023-03-18\"],\n",
    "    'fcm_dir': \"/home/aamad/codice/city-pro/output/bologna_mdt_center\",\n",
    "    'plot_dir': \"/home/aamad/codice/city-pro/output/bologna_mdt_center/plots\",\n",
    "    \n",
    "    'filter_params': {\n",
    "        'filter_length_km': 10,\n",
    "        'filter_time_hours': 1.5,\n",
    "        'filter_space': pl.col(\"lenght_km\") < 10,\n",
    "        'filter_time': pl.col(\"time_hours\") < 1.5,\n",
    "    },\n",
    "    \n",
    "    'distribution_params': {\n",
    "        'bin_size_time_hours': 0.05,\n",
    "        'bin_size_length_km': 1,\n",
    "        'range_time_hours': [0.1, 1.5],\n",
    "        'range_length_km': [0.1, 10],\n",
    "        'enriched_vector_length': 50,\n",
    "        'cut_length': 4,\n",
    "        'window_size': 3,\n",
    "        'case': \"_forced_fits\"  # Add a suffix to distinguish these from other plots\n",
    "    }\n",
    "}\n",
    "\n",
    "fit_params = run_average_distribution_analysis(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Gauss for paper -> still experimental and will remain such since we force to avoid the fite over 40 km/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from PlotSettings import *\n",
    "from analysisPlot import *\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Define Gaussian function\n",
    "def gaussian(x, A, mu, sigma):\n",
    "    return A * np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "\n",
    "def gaussian_fit(x, y):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian function to x,y data points\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        x-coordinates\n",
    "    y : array-like\n",
    "        y-coordinates (observations)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    params : tuple\n",
    "        (A, mu, sigma) parameters of the Gaussian fit\n",
    "        A: amplitude\n",
    "        mu: mean\n",
    "        sigma: standard deviation\n",
    "    covariance : 2D array\n",
    "        Covariance matrix of the parameters\n",
    "    fitted_y : array\n",
    "        y values of the fitted Gaussian at x positions\n",
    "    \"\"\"\n",
    "    # Initial guesses for parameters\n",
    "    A_guess = np.max(y)\n",
    "    mu_guess = np.sum(x * y) / np.sum(y)  # Center of mass\n",
    "    sigma_guess = np.sqrt(np.sum(y * (x - mu_guess)**2) / np.sum(y))\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Perform the fit\n",
    "        params, covariance = curve_fit(\n",
    "            gaussian, x, y, \n",
    "            p0=[A_guess, mu_guess, sigma_guess],\n",
    "            maxfev=50000\n",
    "        )\n",
    "        \n",
    "        # Calculate fitted y values\n",
    "        fitted_y = gaussian(x, *params)\n",
    "        \n",
    "        return params, covariance, fitted_y\n",
    "    \n",
    "    except RuntimeError:\n",
    "        print(\"Error: Fit did not converge\")\n",
    "        return (A_guess, mu_guess, sigma_guess), None, None\n",
    "\n",
    "import json\n",
    "key_2_int_class = {\"1 slowest\":0,\n",
    " \"2 slowest\":1,\n",
    " \"middle velocity class\":2,\n",
    " \"1 quickest\":3}\n",
    "int_class_2_color = {0:\"blue\",\n",
    "                    1:\"orange\",\n",
    "                    2:\"green\",\n",
    "                    3:\"red\"}\n",
    "PlotDir = \"/home/aamad/codice/city-pro/output/bologna_mdt_center/plots/\"\n",
    "with open(\"/home/aamad/codice/city-pro/output/bologna_mdt_center/plots/aggregated_average_speed_kmh_plots.json\",\"r\") as f:\n",
    "    aggregated_average_speed = json.load(f)\n",
    "    aggregated_average_speed = {key:{\"x\":v[\"x\"],\"y\":v[\"y\"]} for key,v in aggregated_average_speed.items()}\n",
    "fig,ax = plt.subplots(1,1,figsize = (10,10))\n",
    "for key in aggregated_average_speed.keys():\n",
    "    int_class = key_2_int_class[key]\n",
    "    # Upload the x,y\n",
    "    x = aggregated_average_speed[key][\"x\"]\n",
    "    y = aggregated_average_speed[key][\"y\"]\n",
    "    Feature = \"speed_kmh\"\n",
    "    Feature2Label = {\"time_hours\":\"t\",\"lenght_km\":\"L\",\"speed_kmh\":\"v (km/h)\"}\n",
    "    # Scatter Points\n",
    "    if int_class == 2:\n",
    "        x_fit = np.array(x)[np.array(x) <= 40]\n",
    "        y_fit = np.array(y)[np.array(x) <= 40]\n",
    "        params, covariance, fitted_y = gaussian_fit(x_fit, y_fit)\n",
    "\n",
    "    else:\n",
    "        x_fit = np.array(x)\n",
    "        y_fit = np.array(y)\n",
    "        params, covariance, fitted_y = gaussian_fit(x_fit, y_fit)    \n",
    "    ax.scatter(x,y,color = int_class_2_color[int_class])\n",
    "    ax.plot(x,gaussian(x,params[0],params[1],params[2]),color = int_class_2_color[int_class])\n",
    "    ax.set_xticks(np.arange(x[0],x[-1],20))\n",
    "    ax.set_yticks(np.arange(min(y),max(y),0.05))\n",
    "    ax.set_xlabel(Feature2Label[Feature])\n",
    "    ax.set_ylabel(\"P(v)\")\n",
    "    ax.set_xlim(left = 0,right = 150)\n",
    "#    ax.set_xscale(Feature2ScaleBins[Feature])\n",
    "#    ax.set_yscale(Feature2ScaleCount[Feature])\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\")\n",
    "PlotInsetSpeedAverage(Feature,ax_inset,PlotDir)\n",
    "plt.savefig(\"/home/aamad/codice/city-pro/output/bologna_mdt_center/plots/aggregated_average_speed_kmh_plots.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Per Condition on The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
